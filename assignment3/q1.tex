\titledquestion{Machine Learning \& Neural Networks}[8] 
\newenvironment{answer}{
    % Uncomment this if using the template to write out your solutions.
    {\bf Answer:} \sf \begingroup\color{red}
}{\endgroup}%
\definecolor{shadecolor}{gray}{0.9}

%\newif\ifanswers
%\answerstrue
\begin{parts}

    
    \part[4] Adam Optimizer\newline
        Recall the standard Stochastic Gradient Descent update rule:
        \alns{
            	\btheta_{t+1} &\gets \btheta_t - \alpha \nabla_{\btheta_t} J_{\text{minibatch}}(\btheta_t)
        }
        where $t+1$ is the current timestep, $\btheta$ is a vector containing all of the model parameters, ($\btheta_t$ is the model parameter at time step $t$, and $\btheta_{t+1}$ is the model parameter at time step $t+1$), $J$ is the loss function, $\nabla_{\btheta} J_{\text{minibatch}}(\btheta)$ is the gradient of the loss function with respect to the parameters on a minibatch of data, and $\alpha$ is the learning rate.
        Adam Optimization\footnote{Kingma and Ba, 2015, \url{https://arxiv.org/pdf/1412.6980.pdf}} uses a more sophisticated update rule with two additional steps.\footnote{The actual Adam update uses a few additional tricks that are less important, but we won't worry about them here. If you want to learn more about it, you can take a look at: \url{http://cs231n.github.io/neural-networks-3/\#sgd}}
            
        \begin{subparts}

            \subpart[2]First, Adam uses a trick called {\it momentum} by keeping track of $\bm$, a rolling average of the gradients:
                \alns{
                	\bm_{t+1} &\gets \beta_1\bm_{t} + (1 - \beta_1)\nabla_{\btheta_t} J_{\text{minibatch}}(\btheta_t) \\
                	\btheta_{t+1} &\gets \btheta_t - \alpha \bm_{t+1}
                }
                where $\beta_1$ is a hyperparameter between 0 and 1 (often set to  0.9). Briefly explain in 2--4 sentences (you don't need to prove mathematically, just give an intuition) how using $\bm$ stops the updates from varying as much and why this low variance may be helpful to learning, overall.\newline
             \begin{shaded}
                \begin{answer}
                The momentum weights the gradient at timestep $t$ with a number less than 1 and sums it with a weighted
                average of the gradients at previous time steps. Therefore, it reduces the contribution of the current
                gradient and so reduces the variation in the direction of traversal of the parameter space. If there are successive gradient
                updates in a certain direction for a coordinate in the parameter space (which i will denote $\theta_k$) then, with a momentum update, the update rule will continue to move
                along that coordinate even if there is some fluctuation in the gradient at each time step with respect to $\theta_k$. This results
                in finding a minimum of the loss function more quickly.
            \end{answer}
             \end{shaded} 
                
            \subpart[2] Adam extends the idea of {\it momentum} with the trick of {\it adaptive learning rates} by keeping track of  $\bv$, a rolling average of the magnitudes of the gradients:
                \alns{
                	\bm_{t+1} &\gets \beta_1\bm_{t} + (1 - \beta_1)\nabla_{\btheta_t} J_{\text{minibatch}}(\btheta_t) \\
                	\bv_{t+1} &\gets \beta_2\bv_{t} + (1 - \beta_2) (\nabla_{\btheta_t} J_{\text{minibatch}}(\btheta_t) \odot \nabla_{\btheta_t} J_{\text{minibatch}}(\btheta_t)) \\
                	\btheta_{t+1} &\gets \btheta_t - \alpha \bm_{t+1} / \sqrt{\bv_{t+1}}
                }
                where $\odot$ and $/$ denote elementwise multiplication and division (so $\bz \odot \bz$ is elementwise squaring) and $\beta_2$ is a hyperparameter between 0 and 1 (often set to  0.99). Since Adam divides the update by $\sqrt{\bv}$, which of the model parameters will get larger updates?  Why might this help with learning?
               \begin{shaded}
                \begin{answer}
                    For a parameter $\theta_k$ in the parameter space, denote by $v_{t,k}$ and $\nabla_{\theta_{t,k}}J$
                    the $k$-th coordinate of $v_t$ and $\nabla_{\theta_t}J_{\text{minibatch}}(\theta_t)$ respectively. If $\sqrt{v_{t+1,k}}$ is less than 1,
                     the magnitude of update for the parameter will be increased. If $\sqrt{v_{t+1,k}}$ is less than 1 both $\beta_2 v_{t,k}$ and $(1-\beta_2)\nabla_{\theta_{t,k}}J$ 
                     must be less than 1. Therefore, larger parameter updates happen for parameters which are relatively infrequently updated.
                    One reason a parameter may not be updated frequently is that non-zero data for that parameter may only occur for a few data points,
                    such as when dealing with a sparse dataset. However, the parameter could be a highly useful predictor when it is observed.
                    Performing large updates for infrequently seen but good predictors could improve performance of the network in this case. 
                \end{answer}
               \end{shaded}
           
                
                \end{subparts}
        
        
            \part[4] 
            Dropout\footnote{Srivastava et al., 2014, \url{https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf}} is a regularization technique. During training, dropout randomly sets units in the hidden layer $\bh$ to zero with probability $p_{\text{drop}}$ (dropping different units each minibatch), and then multiplies $\bh$ by a constant $\gamma$. We can write this as:
                \alns{
                	\bh_{\text{drop}} = \gamma \bd \odot \bh
                }
                where $\bd \in \{0, 1\}^{D_h}$ ($D_h$ is the size of $\bh$)
                is a mask vector where each entry is 0 with probability $p_{\text{drop}}$ and 1 with probability $(1 - p_{\text{drop}})$. $\gamma$ is chosen such that the expected value of $\bh_{\text{drop}}$ is $\bh$:
                \alns{
                	\mathbb{E}_{p_{\text{drop}}}[\bh_\text{drop}]_i = h_i \text{\phantom{aaaa}}
                }
                for all $i \in \{1,\dots,D_h\}$. 
            \begin{subparts}
            \subpart[2]
                What must $\gamma$ equal in terms of $p_{\text{drop}}$? Briefly justify your answer or show your math derivation using the equations given above.
               \begin{shaded}
                \begin{answer}
                    \begin{align*}
                        \mathbb{E}_{p_\text{drop}}[\bh_{\text{drop}}]_i &= p_{drop} \cdot \gamma \cdot 0 + \gamma \cdot (1-p_{drop}) \cdot h_i\\
                                        &= \gamma \cdot (1-p_{drop}) \cdot h_i
                    \end{align*}
                    As $\gamma$ is chosen such that the expectation is $h_i$, $\gamma = \dfrac{1}{1-p_{drop}}$
                \end{answer}
               \end{shaded} 
            
          \subpart[2] Why should dropout be applied during training? Why should dropout \textbf{NOT} be applied during evaluation? (Hint: it may help to look at the paper linked above in the write-up.) \newline
          \begin{shaded}
            \begin{answer}
                As nodes are randomly dropped out, nodes in subsequent layers can not rely on their input being non-zero.
                One intuition why this might be useful, is that this forces nodes to be useful on their own and not rely
                on complicated interdependencies, when a simpler function can be learnt for a predictive task. Using dropout
                during training can be thought of as sampling a "thinned" network from the current network architecture. If
                all nodes are useful for the predictive task, we would not want to use a "thinned" network for testing. One 
                alternative would be to average predictions over a selection of thinned networks. However, if the number of
                selections is large, this would be cost prohibitive.
            \end{answer}
        \end{shaded}
          
         
        \end{subparts}


\end{parts}
